---
title: How To Scale Transformersâ€™ Memory up to 262K Tokens With a Minor Change?
subtitle: Extending Transformers by memorizing up to 262K tokens
summary: This article is a fabulous attempt to leverage language models in memorizing information by transformers with the least required effort. The point is that we can use it for available pre-trained models.
authors:
  - Reza
tags: []
featured: true
categories: []
external_link: https://medium.com/towards-artificial-intelligence/extending-transformers-by-memorizing-up-to-262k-tokens-f9e066108777
reading_time: false
projects: []
date: '2023-02-05T00:00:00Z'
image:
  caption: ''
  focal_point: ''
---
